{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T05:13:09.164852Z",
     "start_time": "2020-05-14T05:12:59.980065Z"
    }
   },
   "outputs": [],
   "source": [
    "from u import *\n",
    "from data import *\n",
    "import model\n",
    "import quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T05:57:37.358032Z",
     "start_time": "2020-05-14T05:57:12.933303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model at step 200000\n",
      "Model has 8304213 parameters. Embedding has 3278270 parameters\n",
      "valid {'loss': 3.676476001739502, 'perplexity': 39.50692613554216, 'time': 2.0}\n",
      "test {'loss': 3.721527099609375, 'perplexity': 41.327457088205485, 'time': 2.0}\n"
     ]
    }
   ],
   "source": [
    "c = Config(Res / 'quantize_prune33.75_distill_8.3M_cache2000_hebbian_step175000_cache3000_bits9',\n",
    "           device='cpu', distributed=False).load()\n",
    "# Evaluate perplexities on validation and test sets\n",
    "# ! cd {c.res} && CUDA_VISIBLE_DEVICES=0 python ../../main.py . valid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T05:09:17.560491Z",
     "start_time": "2020-05-14T05:09:14.603760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded step 200000\n"
     ]
    }
   ],
   "source": [
    "net = eval(c.get('model', 'model.Transformer'))(c)\n",
    "net, step = c.init_model(net, step=c.get('step', 'max'), train=False)\n",
    "print('Loaded step', step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T05:12:18.735243Z",
     "start_time": "2020-05-14T05:12:18.381959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonzero params 73588631\n",
      "total params 73588631\n",
      "target total sparsity 0\n",
      "total sparsity 0\n",
      "\n",
      "total param size 73588631.0\n",
      "total mask size 2299644.71875\n",
      "total quantization size 0\n",
      "total size 75888275.71875\n"
     ]
    }
   ],
   "source": [
    "target_sparsities = {}\n",
    "if (c.res / 'distiller_prune.yaml').exists():\n",
    "    for k, v in (c.res / 'distiller_prune.yaml').load()['pruners'].items():\n",
    "        for w in v['weights']:\n",
    "            target_sparsities[w] = v['final_sparsity']\n",
    "        \n",
    "nonzero_params = 0\n",
    "target_nonzero_params = 0\n",
    "total_params = 0\n",
    "\n",
    "mask_size = 0\n",
    "quantization_size = 0\n",
    "\n",
    "for k, p in net.state_dict().items():\n",
    "    if k.startswith('loss.layers.') and k.endswith('.weight'): # shared with input embedding\n",
    "        continue\n",
    "    param_type = k.split('.')[-1]\n",
    "    if param_type == 'max_abs':\n",
    "        quantization_size += 1\n",
    "    elif param_type == 'inv_scale':\n",
    "        quantization_size += (32 - c.bits) / 32\n",
    "    elif param_type in ['weight', 'bias', 'pos_emb']: # masked params\n",
    "        if '.ln1.' in k or '.ln2.' in k: # ignore layernorm beta, gamma, can be fused into fc\n",
    "            continue\n",
    "        nz = from_torch((p != 0).sum())\n",
    "        total = p.numel()\n",
    "        if total == 0: continue\n",
    "        nonzero_params += nz\n",
    "        total_params += total\n",
    "        mask_size += total / 32\n",
    "        target_nonzero_params += total * (1 - target_sparsities.get(k, 0))\n",
    "#         print(k, 'sparsity %.5g' % (1 - nz / total))\n",
    "    elif param_type in ['cache_theta_inv_softplus', 'cache_lambda_inv_sigmoid']:\n",
    "        nonzero_params += p.numel()\n",
    "        total_params += p.numel()\n",
    "    else:\n",
    "        raise RuntimeError('Should not happen')\n",
    "# print()\n",
    "print('nonzero params', nonzero_params)\n",
    "print('total params', total_params)\n",
    "if len(target_sparsities):\n",
    "    print('target total sparsity %.5g' % (1 - target_nonzero_params / total_params))\n",
    "print('total sparsity %.5g' % (1 - nonzero_params / total_params))\n",
    "\n",
    "print()\n",
    "param_size = nonzero_params * c.get('bits', 32) / 32\n",
    "print('total param size', param_size)\n",
    "print('total mask size', mask_size)\n",
    "print('total quantization size', quantization_size)\n",
    "total_size = param_size + mask_size + quantization_size\n",
    "print('total size', total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T05:09:18.250583Z",
     "start_time": "2020-05-14T05:09:17.882369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[245569.]\n",
      "matrix multiplications 74097600.0\n",
      "matrix multiplications after quantization 74097600.0\n",
      "additions 74081664.0\n",
      "others fp32 840083\n",
      "total 149019347.0\n"
     ]
    }
   ],
   "source": [
    "test_tokens = (Cache / 'wikitext-103/sorted_test.npy').load()\n",
    "cutoffs = np.array([0] + c.cutoffs + [c.n_vocab])\n",
    "token_bin_counts = np.zeros(len(c.cutoffs) + 1)\n",
    "for i, (prev_cutoff, cutoff) in enumerate(zip(cutoffs, cutoffs[1:])):\n",
    "    token_bin_counts[i] = ((prev_cutoff <= test_tokens) & (test_tokens < cutoff)).sum()\n",
    "print(token_bin_counts)\n",
    "token_bin_fracs = token_bin_counts / token_bin_counts.sum()\n",
    "\n",
    "# cumulative operation counts\n",
    "muls = 0\n",
    "adds = 0\n",
    "others = 0\n",
    "\n",
    "def density(p):\n",
    "    return from_torch((p != 0).sum()) / (p.numel() or 1)\n",
    "\n",
    "def tally_fc(fc, multiplier=1):\n",
    "    wd = density(fc.weight)\n",
    "    bd = density(fc.bias) if fc.bias is not None else 0\n",
    "    out, in_ = fc.weight.shape\n",
    "    tally_matmul(in_, out, multiplier * wd, multiplier * bd)\n",
    "    \n",
    "def tally_matmul(in_, out, mmultiplier=1, bmultiplier=1):\n",
    "    global muls, adds\n",
    "    # multiplies a matrix of shape (out, in_) by a vector of shape (in_,) with optional bias\n",
    "    muls += mmultiplier * in_ * out\n",
    "    # the intuition is that in_ is the dimension that's being summed up\n",
    "    adds += mmultiplier * (in_ - 1) * out + bmultiplier * out\n",
    "    \n",
    "def tally_layernorm(ln):\n",
    "    global others\n",
    "    dim = np.prod(l.ln1.weight.shape)\n",
    "    others += dim + dim * 2 + (dim - 1) + 1 + dim + dim\n",
    "    \n",
    "def tally_softmax(dim):\n",
    "    global others\n",
    "    others += dim + (dim - 1) + dim\n",
    "    \n",
    "for f, p in zip(token_bin_fracs[1:], net.embed.projections):\n",
    "    tally_fc(p, multiplier=f)\n",
    "    \n",
    "for l in net.layers:\n",
    "    # layer norm 1\n",
    "    tally_layernorm(l.ln1)\n",
    "\n",
    "    # qkv fully connected layer\n",
    "    tally_fc(l.qkv)\n",
    "    \n",
    "    # q * k\n",
    "    context = c.n_seq + 1\n",
    "    tally_matmul(c.n_k, context * c.n_head, bmultiplier=0)\n",
    "    \n",
    "    # q * pos_emb + qk\n",
    "    tally_matmul(c.n_k, context)\n",
    "    \n",
    "    # softmax\n",
    "    tally_softmax(context)\n",
    "    \n",
    "    # attn * v\n",
    "    tally_matmul(context, c.n_head * c.n_v, bmultiplier=0)\n",
    "    \n",
    "    # out fully connected layer\n",
    "    tally_fc(l.out)\n",
    "    \n",
    "    # residual\n",
    "    adds += c.n_embed\n",
    "    \n",
    "    # layer norm 2\n",
    "    tally_layernorm(l.ln2)\n",
    "    \n",
    "    # FFN 1\n",
    "    tally_fc(l.fc[0])\n",
    "    \n",
    "    # ReLU\n",
    "    others += c.n_inner\n",
    "    \n",
    "    # FFN 2\n",
    "    tally_fc(l.fc[3])\n",
    "    \n",
    "    # residual\n",
    "    adds += c.n_embed\n",
    "\n",
    "# first bin\n",
    "tally_fc(net.loss.clusters)\n",
    "\n",
    "# projections for other bins\n",
    "for layer in net.loss.layers:\n",
    "    tally_fc(layer)\n",
    "for p in net.loss.projections:\n",
    "    tally_fc(p)\n",
    "\n",
    "# softmax over different bins\n",
    "bin_sizes = cutoffs[1:] - cutoffs[:-1]\n",
    "bin_sizes[0] += len(bin_sizes[1:])\n",
    "\n",
    "for s in bin_sizes:\n",
    "    tally_softmax(s)\n",
    "\n",
    "# cache\n",
    "tally_matmul(c.n_embed, c.get('n_cache', 0), bmultiplier=0)\n",
    "tally_softmax(c.get('n_cache', 0))\n",
    "\n",
    "\n",
    "muls_quant = muls * c.get('bits', 32) / 32\n",
    "\n",
    "print('matrix multiplications', muls)\n",
    "print('matrix multiplications after quantization', muls_quant)\n",
    "print('additions', adds)\n",
    "print('others fp32', others)\n",
    "total_flops = muls_quant + adds + others\n",
    "print('total', total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:53:25.969279Z",
     "start_time": "2020-05-14T00:53:25.965257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score 0.038746852160481786\n"
     ]
    }
   ],
   "source": [
    "print('final score', total_size / 159e6 + total_flops / 318e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
