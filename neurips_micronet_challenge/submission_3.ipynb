{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T19:47:08.363126Z",
     "start_time": "2019-12-13T19:46:55.167281Z"
    }
   },
   "outputs": [],
   "source": [
    "from u import *\n",
    "from ut import *\n",
    "from data import *\n",
    "import quantized_model\n",
    "from quantized_model import evaluate, get_net\n",
    "quantized_model.distiller_vs_explicit = 'explicit' # switch off using distiller, which we use during quantization aware training\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "num_bits = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T19:47:13.893312Z",
     "start_time": "2019-12-13T19:47:08.365839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): AdaptiveEmbedding(\n",
       "    (layers): ModuleList(\n",
       "      (0): Embedding(3500, 256)\n",
       "      (1): Embedding(21500, 64)\n",
       "      (2): Embedding(242735, 4)\n",
       "    )\n",
       "    (projections): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=False)\n",
       "      (1): Linear(in_features=4, out_features=256, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout1): Dropout(p=0)\n",
       "  (layers): ModuleList(\n",
       "    (0): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (3): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (4): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (5): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (6): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (7): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant): ExplicitQuantize()\n",
       "  (dropout2): Dropout(p=0)\n",
       "  (loss): ProjectedAdaptiveLogSoftmax(\n",
       "    (clusters): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=3500, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=21500, bias=True)\n",
       "      (2): Linear(in_features=4, out_features=242735, bias=True)\n",
       "    )\n",
       "    (projections): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=False)\n",
       "      (1): Linear(in_features=256, out_features=4, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in hyperparameters and paths, etc\n",
    "c = Config(Wiki / 'quant_aware,3', device='cuda:0', distributed=False).load()\n",
    "\n",
    "# load in data\n",
    "data_val = SequentialIterator(c, c.eval_batch, split='valid')\n",
    "data_test = SequentialIterator(c, c.eval_batch, split='test')\n",
    "\n",
    "# create network\n",
    "net = get_net(c)\n",
    "net.load_state_dict(\n",
    "    torch.load(c.res / 'models/model-1-processed.pth')\n",
    ")\n",
    "net = net.to(c.device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T06:16:48.986289Z",
     "start_time": "2019-10-12T06:16:44.392777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: {'loss': 3.5345168113708496, 'perplexity': 34.278447755887655, 'time': 2.0}\n",
      "test: {'loss': 3.553952693939209, 'perplexity': 34.95119619328556, 'time': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "net.cache_keys = net.cache_values = None\n",
    "print('validation:', evaluate(c, data_val, net)) \n",
    "\n",
    "# test\n",
    "net.cache_keys = net.cache_values = None\n",
    "print('test:', evaluate(c, data_test, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T19:47:13.954936Z",
     "start_time": "2019-12-13T19:47:13.897260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.layers.0.weight sparsity 0.1448\n",
      "embed.layers.1.weight sparsity 0.26571\n",
      "embed.layers.2.weight sparsity 0.32646\n",
      "embed.projections.0.weight sparsity 0.42346\n",
      "embed.projections.1.weight sparsity 0.0048828\n",
      "layers.0.pos_emb sparsity 0\n",
      "layers.0.qkv.weight sparsity 0.32646\n",
      "layers.0.qkv.bias sparsity 0.10938\n",
      "layers.0.out.weight sparsity 0.45571\n",
      "layers.0.fc.0.weight sparsity 0.30331\n",
      "layers.0.fc.0.bias sparsity 0\n",
      "layers.0.fc.3.weight sparsity 0.23794\n",
      "layers.0.fc.3.bias sparsity 0.078125\n",
      "layers.1.pos_emb sparsity 0\n",
      "layers.1.qkv.weight sparsity 0.43719\n",
      "layers.1.qkv.bias sparsity 0.064236\n",
      "layers.1.out.weight sparsity 0.612\n",
      "layers.1.fc.0.weight sparsity 0.38183\n",
      "layers.1.fc.0.bias sparsity 0.0013021\n",
      "layers.1.fc.3.weight sparsity 0.3496\n",
      "layers.1.fc.3.bias sparsity 0.066406\n",
      "layers.2.pos_emb sparsity 0\n",
      "layers.2.qkv.weight sparsity 0.35868\n",
      "layers.2.qkv.bias sparsity 0.10764\n",
      "layers.2.out.weight sparsity 0.54348\n",
      "layers.2.fc.0.weight sparsity 0.37274\n",
      "layers.2.fc.0.bias sparsity 0\n",
      "layers.2.fc.3.weight sparsity 0.47423\n",
      "layers.2.fc.3.bias sparsity 0.027344\n",
      "layers.3.pos_emb sparsity 0\n",
      "layers.3.qkv.weight sparsity 0.42348\n",
      "layers.3.qkv.bias sparsity 0.076389\n",
      "layers.3.out.weight sparsity 0.5981\n",
      "layers.3.fc.0.weight sparsity 0.37274\n",
      "layers.3.fc.0.bias sparsity 0.0013021\n",
      "layers.3.fc.3.weight sparsity 0.49274\n",
      "layers.3.fc.3.bias sparsity 0.023438\n",
      "layers.4.pos_emb sparsity 0\n",
      "layers.4.qkv.weight sparsity 0.40497\n",
      "layers.4.qkv.bias sparsity 0.059028\n",
      "layers.4.out.weight sparsity 0.52513\n",
      "layers.4.fc.0.weight sparsity 0.40497\n",
      "layers.4.fc.0.bias sparsity 0.0052083\n",
      "layers.4.fc.3.weight sparsity 0.42348\n",
      "layers.4.fc.3.bias sparsity 0.035156\n",
      "layers.5.pos_emb sparsity 0\n",
      "layers.5.qkv.weight sparsity 0.40497\n",
      "layers.5.qkv.bias sparsity 0.074653\n",
      "layers.5.out.weight sparsity 0.54348\n",
      "layers.5.fc.0.weight sparsity 0.40497\n",
      "layers.5.fc.0.bias sparsity 0.0026042\n",
      "layers.5.fc.3.weight sparsity 0.40497\n",
      "layers.5.fc.3.bias sparsity 0.019531\n",
      "layers.6.pos_emb sparsity 0\n",
      "layers.6.qkv.weight sparsity 0.42348\n",
      "layers.6.qkv.bias sparsity 0.069444\n",
      "layers.6.out.weight sparsity 0.612\n",
      "layers.6.fc.0.weight sparsity 0.3496\n",
      "layers.6.fc.0.bias sparsity 0.0026042\n",
      "layers.6.fc.3.weight sparsity 0.3496\n",
      "layers.6.fc.3.bias sparsity 0.015625\n",
      "layers.7.pos_emb sparsity 0\n",
      "layers.7.qkv.weight sparsity 0.32646\n",
      "layers.7.qkv.bias sparsity 0.038194\n",
      "layers.7.out.weight sparsity 0.42348\n",
      "layers.7.fc.0.weight sparsity 0.23794\n",
      "layers.7.fc.0.bias sparsity 0.014323\n",
      "layers.7.fc.3.weight sparsity 0.30331\n",
      "layers.7.fc.3.bias sparsity 0.027344\n",
      "loss.clusters.weight sparsity 0.019531\n",
      "loss.clusters.bias sparsity 0\n",
      "loss.layers.0.bias sparsity 0.016571\n",
      "loss.layers.1.bias sparsity 0.9033\n",
      "loss.layers.2.bias sparsity 0.58102\n",
      "loss.projections.0.weight sparsity 0.32642\n",
      "loss.projections.1.weight sparsity 0.010742\n",
      "\n",
      "nonzero params 5487556\n",
      "total params 8296023\n",
      "total sparsity 0.33853\n",
      "\n",
      "total param size 1371889.0\n",
      "total mask size 259250.65625\n",
      "total quantization size 99.75\n",
      "total size 1631239.40625\n"
     ]
    }
   ],
   "source": [
    "nonzero_params = 0\n",
    "total_params = 0\n",
    "\n",
    "mask_size = 0\n",
    "quantization_size = 0\n",
    "\n",
    "for k, p in net.state_dict().items():\n",
    "    if k in ['loss.layers.0.weight', 'loss.layers.1.weight', 'loss.layers.2.weight']: # shared with input embedding\n",
    "        continue\n",
    "    param_type = k.split('.')[-1]\n",
    "    if param_type == 'max_abs':\n",
    "        quantization_size += 1\n",
    "    elif param_type == 'inv_scale':\n",
    "        quantization_size += (32 - num_bits) / 32\n",
    "    elif param_type in ['weight', 'bias', 'pos_emb']: # masked params\n",
    "        if '.ln1.' in k or '.ln2.' in k: # ignore layernorm beta, gamma, can be fused into fc\n",
    "            continue\n",
    "        nz = from_torch((p != 0).sum())\n",
    "        total = p.numel()\n",
    "        nonzero_params += nz\n",
    "        total_params += total\n",
    "        mask_size += total / 32\n",
    "        print(k, 'sparsity %.5g' % (1 - nz / total))\n",
    "    elif param_type in ['cache_theta_inv_softplus', 'cache_lambda_inv_sigmoid']:\n",
    "        nonzero_params += p.numel()\n",
    "        total_params += p.numel()\n",
    "    else:\n",
    "        raise RuntimeError('Should not happen')\n",
    "print()\n",
    "print('nonzero params', nonzero_params)\n",
    "print('total params', total_params)\n",
    "print('total sparsity %.5g' % (1 - nonzero_params / total_params))\n",
    "\n",
    "print()\n",
    "param_size = nonzero_params * num_bits / 32\n",
    "print('total param size', param_size)\n",
    "print('total mask size', mask_size)\n",
    "print('total quantization size', quantization_size)\n",
    "total_size = param_size + mask_size + quantization_size\n",
    "print('total size', total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T19:47:21.441520Z",
     "start_time": "2019-12-13T19:47:21.398580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplications 6168904.932279726\n",
      "matrix multiplications after quantization 1542226.2330699314\n",
      "additions 6091477.687298576\n",
      "others fp32 842247\n",
      "total 8475950.920368508\n"
     ]
    }
   ],
   "source": [
    "params = net.state_dict()\n",
    "densities = {}\n",
    "\n",
    "matmuls = 0\n",
    "adds = 0\n",
    "others = 0\n",
    "\n",
    "# collect densities\n",
    "for k, p in params.items():\n",
    "    param_type = k.split('.')[-1]\n",
    "    if param_type == 'max_abs': # TODO\n",
    "        pass\n",
    "    elif param_type == 'inv_scale':\n",
    "        pass\n",
    "    elif param_type in ['weight', 'bias']:\n",
    "        nz = from_torch((p != 0).sum())\n",
    "        total = p.numel()\n",
    "        densities[k] = nz / total\n",
    "\n",
    "# input embedding\n",
    "token_bin_counts = np.array([198232, 35479, 11858])\n",
    "token_bin_fracs = token_bin_counts / token_bin_counts.sum()\n",
    "for i, p in enumerate(token_bin_fracs):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    embed_weight = params['embed.layers.%s.weight' % i] \n",
    "    proj_weight = params['embed.projections.%s.weight' % (i - 1)]\n",
    "    proj_density = densities['embed.projections.%s.weight' % (i - 1)]\n",
    "    \n",
    "    h2, h1 = proj_weight.shape\n",
    "    matmuls += h1 * h2 * p * proj_density\n",
    "    adds += (h1 - 1) * h2 * p * proj_density\n",
    "\n",
    "n_layers = 8\n",
    "n_hidden = 256\n",
    "\n",
    "for i in range(n_layers):\n",
    "    layer_matmuls = 0\n",
    "    layer_adds = 0\n",
    "    \n",
    "    # layer norm 1\n",
    "    ln1 = n_hidden + n_hidden * 2 + (n_hidden - 1) + 1 + n_hidden + n_hidden\n",
    "    \n",
    "    # qkv fully connected layer\n",
    "    w_density = densities['layers.%s.qkv.weight' % i]\n",
    "    b_density = densities['layers.%s.qkv.bias' % i]\n",
    "    layer_matmuls += 256 * 192 * 3 * w_density\n",
    "    layer_adds += (256 - 1) * 192 * 3 * w_density + 192 * 3 * b_density\n",
    "    \n",
    "    # q * k\n",
    "    layer_matmuls += 24 * 8 * 97\n",
    "    layer_adds += (24 - 1) * 8 * 97\n",
    "    \n",
    "    # positional embedding\n",
    "    layer_matmuls += 24 * 97\n",
    "    layer_adds += (24 - 1) * 97 + 97\n",
    "    \n",
    "    # softmax\n",
    "    sm = 97 + 97 - 1 + 97\n",
    "    \n",
    "    # attn * v\n",
    "    layer_matmuls += 97 * 24 * 8\n",
    "    layer_adds += (97 - 1) * 24 * 8\n",
    "    \n",
    "    # out fully connected layer\n",
    "    w_density = densities['layers.%s.out.weight' % i]\n",
    "    layer_matmuls += 192 * 256 * w_density\n",
    "    layer_adds += (192 - 1) * 256 * w_density\n",
    "    \n",
    "    # residual\n",
    "    layer_adds += 256\n",
    "    \n",
    "    # layer norm 2\n",
    "    ln2 = ln1\n",
    "    \n",
    "    # FFN 1\n",
    "    w_density = densities['layers.%s.fc.0.weight' % i]\n",
    "    b_density = densities['layers.%s.fc.0.bias' % i]\n",
    "    layer_matmuls += 256 * 768 * w_density\n",
    "    layer_adds += (256 - 1) * 768 * w_density + 768 * b_density\n",
    "    \n",
    "    # ReLU\n",
    "    relu = 768\n",
    "    \n",
    "    w_density = densities['layers.%s.fc.3.weight' % i]\n",
    "    b_density = densities['layers.%s.fc.3.bias' % i]\n",
    "    layer_matmuls += 768 * 256 * w_density\n",
    "    layer_adds += (768 - 1) * 256 * w_density + 256 * b_density\n",
    "    \n",
    "    # residual\n",
    "    layer_adds += 256\n",
    "    \n",
    "    matmuls += layer_matmuls\n",
    "    adds += layer_adds\n",
    "    others += ln1 + sm + ln2 + relu\n",
    "\n",
    "# clusters\n",
    "w_density = densities['loss.clusters.weight']\n",
    "b_density = densities['loss.clusters.bias']\n",
    "matmuls += 256 * 2 * w_density\n",
    "adds += (256 - 1) * 2 * w_density + 2 * b_density\n",
    "\n",
    "# projections for bin 1 and 2\n",
    "for i in range(1, 3):\n",
    "    w_density = densities['loss.projections.%s.weight' % (i - 1)]\n",
    "    matmuls += {0: 256 * 64, 1: 256 * 4}[i - 1] * w_density\n",
    "    adds += {0: (256 - 1) * 64, 1: (256 - 1) * 4}[i - 1] * w_density\n",
    "\n",
    "for i in range(3):\n",
    "    w = params['loss.layers.%s.weight' % i]\n",
    "    w_density = densities['loss.layers.%s.weight' % i]\n",
    "    b_density = densities['loss.layers.%s.bias' % i]\n",
    "    \n",
    "    v, h = w.shape\n",
    "    matmuls += v * h * w_density\n",
    "    adds += (h - 1) * v * w_density + v * b_density\n",
    "sm = sum([x * 3 - 1 for x in (3502, 21500, 242735)])\n",
    "others += sm\n",
    "\n",
    "matmuls += 256 * 2000\n",
    "adds += (256 - 1) * 2000\n",
    "cache_sm = 2000 * 3 - 1\n",
    "others += cache_sm\n",
    "\n",
    "mm_quantized = matmuls * num_bits / 32\n",
    "\n",
    "print('matrix multiplications', matmuls)\n",
    "print('matrix multiplications after quantization', mm_quantized)\n",
    "print('additions', adds)\n",
    "print('others fp32', others)\n",
    "total_flops = mm_quantized + adds + others\n",
    "print('total', total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T19:47:24.166411Z",
     "start_time": "2019-12-13T19:47:24.159725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score 0.03691330104675632\n"
     ]
    }
   ],
   "source": [
    "print('final score', total_size / 159e6 + total_flops / 318e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distiller",
   "language": "python",
   "name": "distiller"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
