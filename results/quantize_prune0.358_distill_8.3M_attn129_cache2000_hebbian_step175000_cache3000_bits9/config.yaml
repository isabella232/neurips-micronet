bits: 9
cache_lambda: 0.1385294212513973
cache_theta: 0.019613149630808832
compress: distiller_quantize.yaml
cutoffs:
- 3500
- 25000
dropout: 0
eval_batch: 1
eval_chunk: 4096
lr: 0.0001
model: quantize.Transformer
n_cache: 3000
n_embed: 256
n_embeds:
- 256
- 64
- 4
n_head: 8
n_inner: 768
n_k: 24
n_layers: 8
n_seq: 128
n_v: 24
n_vocab: 267735
opt_level: O0
pos_emb: trained
scheduler: cosine
step_eval: 1
step_save: 25000
step_warmup: 0
steps: 1
steps_per_epoch: 1000
train_batch: 1
train_chunk: 1152
use_cache: true
